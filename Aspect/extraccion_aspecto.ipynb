{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limpia\n",
      "Aspecto: habitación, Polaridad: positiva\n",
      "sabrosa\n",
      "Aspecto: comida, Polaridad: positiva\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Define la oración que deseas analizar\n",
    "texto = \"La habitación limpia y fresca. La humedad de la piscina era desagradable. La comida muy sabrosa y el servicio estaba horrible\"\n",
    "\n",
    "# Procesamiento de la oración con spaCy\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Bucle para extraer aspectos y polaridades\n",
    "for sentencia in doc.sents:\n",
    "    polaridades = []\n",
    "    for token in sentencia:\n",
    "        if token.dep_ in ('amod', 'acomp'):  # Adjetivos modificativos y predicativos\n",
    "            print(token)\n",
    "            aspecto = token.head.text\n",
    "            polaridad = \"positiva\" if token.sentiment >= 0 else \"negativa\"\n",
    "            print(f\"Aspecto: {aspecto}, Polaridad: {polaridad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra que le aporta un sentimiento a 'habitacion' es: genial\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de lenguaje en español de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Oración dada\n",
    "oracion = \"La habitacion en la que los quedamos era genial.\"\n",
    "\n",
    "# Procesar la oración con SpaCy\n",
    "doc = nlp(oracion)\n",
    "\n",
    "# Buscar la palabra que aporta un sentimiento a \"habitación\"\n",
    "palabra_sentimiento = None\n",
    "for token in doc:\n",
    "    if token.pos_ == \"ADJ\":  # Verificar si es un adjetivo\n",
    "        palabra_sentimiento = token.text\n",
    "        break\n",
    "\n",
    "# Imprimir la palabra que aporta un sentimiento a \"habitación\" si se encuentra\n",
    "if palabra_sentimiento:\n",
    "    print(f\"La palabra que le aporta un sentimiento a 'habitacion' es: {palabra_sentimiento}\")\n",
    "else:\n",
    "    print(\"No se encontraron palabras que aporten sentimientos a 'habitacion' en la oración dada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjetivos para 'habitacion': \n",
      "Adjetivos para 'piscina': \n",
      "Adjetivos para 'comida': sabrosa, horrible\n",
      "Adjetivos para 'servicio': \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de lenguaje en español de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Lista de palabras\n",
    "palabras = [\"habitacion\", \"piscina\", \"comida\", \"servicio\"]\n",
    "\n",
    "# Texto\n",
    "texto = \"La habitación limpia y fresca. La humedad de la piscina era desagradable. La comida muy sabrosa y el servicio estaba horrible\"\n",
    "\n",
    "# Procesar el texto con SpaCy\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Adjetivos por palabra\n",
    "adjetivos_por_palabra = {}\n",
    "\n",
    "# Iterar sobre cada palabra en la lista\n",
    "for palabra in palabras:\n",
    "    adjetivos_palabra = []\n",
    "    \n",
    "    # Buscar adjetivos para cada palabra en el texto\n",
    "    for token in doc:\n",
    "        if token.text == palabra:\n",
    "            for child in token.children:\n",
    "                if child.pos_ == \"ADJ\":\n",
    "                    adjetivos_palabra.append(child.text)\n",
    "    \n",
    "    adjetivos_por_palabra[palabra] = adjetivos_palabra\n",
    "\n",
    "# Imprimir los adjetivos encontrados para cada palabra\n",
    "for palabra, adjetivos in adjetivos_por_palabra.items():\n",
    "    print(f\"Adjetivos para '{palabra}': {', '.join(adjetivos)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zorro --- rápido\n",
      "zorro --- marrón\n",
      "perro --- perezoso\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "objeto_modificado = []\n",
    "modificador = []\n",
    "# Cargar el modelo de lenguaje en español de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Definir la oración de entrada\n",
    "sentence = \"El rápido zorro marrón saltó sobre el perro perezoso.La habitación es limpia y fresca\"\n",
    "\n",
    "# Procesar la oración utilizando SpaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Identificar objeto modificado y su modificador\n",
    "for token in doc:\n",
    "    if ((token.dep_ == \"amod\")  and (token.head.dep_ == \"nsubj\" or token.head.dep_ == \"ROOT\" ))or(token.dep_ == \"amod\" and token.head.dep_ == \"obl\"):\n",
    "        modificador.append(token.text)\n",
    "        objeto_modificado.append(token.head.text)\n",
    "\n",
    "for i in range(len(objeto_modificado)):\n",
    "    print(objeto_modificado[i], \"---\", modificador[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39mLas \u001b[31mdet \u001b[34mhabitaciones \u001b[32mDET\n",
      "\u001b[39mhabitaciones \u001b[31mnsubj \u001b[34mviejas \u001b[32mNOUN\n",
      "\u001b[39meran \u001b[31mcop \u001b[34mviejas \u001b[32mAUX\n",
      "\u001b[39mviejas \u001b[31mROOT \u001b[34mviejas \u001b[32mADJ\n",
      "\u001b[39my \u001b[31mcc \u001b[34msucias \u001b[32mCCONJ\n",
      "\u001b[39msucias \u001b[31mconj \u001b[34mviejas \u001b[32mADJ\n",
      "\u001b[39my \u001b[31mcc \u001b[34mconfort \u001b[32mCCONJ\n",
      "\u001b[39mdu \u001b[31mobj \u001b[34mconfort \u001b[32mPROPN\n",
      "\u001b[39mconfort \u001b[31mconj \u001b[34mviejas \u001b[32mVERB\n",
      "\u001b[39mes \u001b[31mcop \u001b[34mnulo \u001b[32mAUX\n",
      "\u001b[39mnulo \u001b[31mconj \u001b[34mviejas \u001b[32mADJ\n",
      "\u001b[39m. \u001b[31mpunct \u001b[34mviejas \u001b[32mPUNCT\n",
      "\u001b[39m\n",
      " \u001b[31mdep \u001b[34m. \u001b[32mSPACE\n",
      "\u001b[39mLas \u001b[31mdet \u001b[34mzonas \u001b[32mDET\n",
      "\u001b[39mzonas \u001b[31mnsubj \u001b[34mpreciosas \u001b[32mNOUN\n",
      "\u001b[39mcomunes \u001b[31mamod \u001b[34mzonas \u001b[32mADJ\n",
      "\u001b[39mson \u001b[31mcop \u001b[34mpreciosas \u001b[32mAUX\n",
      "\u001b[39mpreciosas \u001b[31mROOT \u001b[34mpreciosas \u001b[32mADJ\n",
      "\u001b[39meso \u001b[31mnsubj \u001b[34mpreciosas \u001b[32mPRON\n",
      "\u001b[39msi \u001b[31mmark \u001b[34mpreciosas \u001b[32mSCONJ\n",
      "\u001b[39m! \u001b[31mpunct \u001b[34mpreciosas \u001b[32mPUNCT\n",
      "\u001b[39mPero \u001b[31madvmod \u001b[34mopciones \u001b[32mCCONJ\n",
      "\u001b[39mno \u001b[31madvmod \u001b[34mopciones \u001b[32mADV\n",
      "\u001b[39mes \u001b[31mcop \u001b[34mopciones \u001b[32mAUX\n",
      "\u001b[39mla \u001b[31mdet \u001b[34mopciones \u001b[32mDET\n",
      "\u001b[39mmejor \u001b[31mamod \u001b[34mopciones \u001b[32mADJ\n",
      "\u001b[39mopciones \u001b[31mROOT \u001b[34mopciones \u001b[32mNOUN\n",
      "\u001b[39mpara \u001b[31mmark \u001b[34malojarse \u001b[32mADP\n",
      "\u001b[39malojarse \u001b[31macl \u001b[34mopciones \u001b[32mVERB\n",
      "\u001b[39ma \u001b[31mcase \u001b[34mhora \u001b[32mADP\n",
      "\u001b[39mla \u001b[31mdet \u001b[34mhora \u001b[32mDET\n",
      "\u001b[39mhora \u001b[31mobl \u001b[34malojarse \u001b[32mNOUN\n",
      "\u001b[39mde \u001b[31mmark \u001b[34mconocer \u001b[32mADP\n",
      "\u001b[39mconocer \u001b[31mxcomp \u001b[34malojarse \u001b[32mVERB\n",
      "\u001b[39muna \u001b[31mdet \u001b[34mciudad \u001b[32mDET\n",
      "\u001b[39mciudad \u001b[31mobj \u001b[34mconocer \u001b[32mNOUN\n",
      "\u001b[39mtan \u001b[31madvmod \u001b[34mbonita \u001b[32mADV\n",
      "\u001b[39mbonita \u001b[31mamod \u001b[34mciudad \u001b[32mADJ\n",
      "\u001b[39mcomo \u001b[31mmark \u001b[34mHaba \u001b[32mSCONJ\n",
      "\u001b[39mLa \u001b[31mdet \u001b[34mHaba \u001b[32mDET\n",
      "\u001b[39mHaba \u001b[31mnmod \u001b[34mciudad \u001b[32mPROPN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from colorama import Fore\n",
    "\n",
    "BLUE = Fore.BLUE\n",
    "RED = Fore.RED\n",
    "RESET = Fore.RESET\n",
    "GREEN = Fore.GREEN\n",
    "\n",
    "# Cargar el modelo pre-entrenado en español de spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Definir la oración de ejemplo\n",
    "sentence = \"\"\"Las habitaciones eran viejas y sucias y du confort es nulo.\n",
    "Las zonas comunes son preciosas eso si! Pero no es la mejor opciones para alojarse a la hora de conocer una ciudad tan bonita como La Haba\"\"\"\n",
    "\n",
    "# Procesar la oración con spacy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Imprimir las dependencias sintácticas de cada token en la oración\n",
    "for token in doc:\n",
    "    print(RESET + token.text, RED + token.dep_, BLUE + token.head.text, GREEN + token.pos_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opción --- mejor\n",
      "hotel --- encuentra\n",
      "zonas --- preciosas\n",
      "zonas --- comunes\n",
      "eso --- preciosas\n",
      "opciones --- mejor\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "objeto_modificado = []\n",
    "modificador = []\n",
    "# Cargar el modelo de lenguaje en español de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Definir la oración de entrada\n",
    "sentence =\"\"\"Como digo en el titulo no merece la pena alojarse en el hotel, la mejor opción es tomarse un cafe o hacer el tour del hotel para conocer su historia.\n",
    "El hotel se encuentra lejos de todo, al centro son mínimo 10€ para ir y otros 10€ para volver.\n",
    "Las habitaciones son super viejas y sucias y du confort es nulo.\n",
    "Las zonas comunes son preciosas eso si! Pero no es la mejor opciones para alojarse a la hora de conocer una ciudad tan bonita como La Habana.\"\"\"\n",
    "\n",
    "# Procesar la oración utilizando SpaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Identificar objeto modificado y su modificador\n",
    "for token in doc:\n",
    "    if token.dep_ == \"nsubj\" and  token.head.dep_ == \"ROOT\" :\n",
    "        modificador.append(token.head.text)\n",
    "        objeto_modificado.append(token.text)\n",
    "    elif token.dep_ == \"nmod\" and  token.head.dep_ == \"nsubj\" :\n",
    "        modificador.append(token.head.text)\n",
    "        objeto_modificado.append(token.text)\n",
    "    elif token.dep_ == \"nsubj\" and  token.head.dep_ == \"conj\" :\n",
    "        modificador.append(token.head.text)\n",
    "        objeto_modificado.append(token.text)\n",
    "    elif token.dep_ == \"amod\" and  token.head.dep_ == \"nsubj\" :\n",
    "        modificador.append(token.text)\n",
    "        objeto_modificado.append( token.head.text)\n",
    "    elif token.dep_ == \"amod\" and  token.head.dep_ == \"obl\" :\n",
    "        modificador.append(token.text)\n",
    "        objeto_modificado.append( token.head.text)\n",
    "    elif token.dep_ == \"amod\" and  token.head.dep_ == \"ROOT\" :\n",
    "        modificador.append(token.text)\n",
    "        objeto_modificado.append( token.head.text)\n",
    "    elif token.dep_ == \"amod\" and  token.head.dep_ == \"conj\" :\n",
    "        modificador.append(token.text)\n",
    "        objeto_modificado.append( token.head.text)\n",
    "\n",
    "\n",
    "for i in range(len(objeto_modificado)):\n",
    "    print(objeto_modificado[i], \"---\", modificador[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digo no merece alojarse mejor tomarse hacer conocer\n",
      "encuentra lejos mínimo ir volver\n",
      "super viejas sucias confort nulo\n",
      "comunes preciosas\n",
      "no mejor alojarse conocer tan bonita\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de lenguaje en español de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"\"\"Como digo en el titulo no merece la pena alojarse en el hotel, la mejor opción es tomarse un cafe o hacer el tour del hotel para conocer su historia.\n",
    "El hotel se encuentra lejos de todo, al centro son mínimo 10€ para ir y otros 10€ para volver.\n",
    "Las habitaciones son super viejas y sucias y du confort es nulo.\n",
    "Las zonas comunes son preciosas eso si! Pero no es la mejor opciones para alojarse a la hora de conocer una ciudad tan bonita como La Habana.\"\"\"\n",
    "\n",
    "# Procesar el texto con SpaCy\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Extraer segmentos que expresan opiniones\n",
    "segmentos_opinion = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    opinion = \"\"\n",
    "    for token in sent:\n",
    "        if token.pos_ in [\"ADJ\", \"ADV\", \"VERB\"]:\n",
    "            opinion += token.text + \" \"\n",
    "    if opinion:\n",
    "        segmentos_opinion.append(opinion.strip())\n",
    "\n",
    "# Imprimir los segmentos que expresan opiniones\n",
    "for segmento in segmentos_opinion:\n",
    "    print(segmento)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los -> DET -> det\n",
      "niños -> NOUN -> nsubj\n",
      "les -> PRON -> obj\n",
      "encantan -> VERB -> ROOT\n",
      "las -> DET -> det\n",
      "galletas -> NOUN -> nsubj\n",
      "de -> ADP -> case\n",
      "crema -> NOUN -> nmod\n",
      ". -> PUNCT -> punct\n",
      "niños\n",
      "galletas\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo de lenguaje en español de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"\"\"Como digo en el titulo no merece la pena alojarse en el hotel, la mejor opción es tomarse un cafe o hacer el tour del hotel para conocer su historia.\n",
    "El hotel se encuentra lejos de todo, al centro son mínimo 10€ para ir y otros 10€ para volver.\n",
    "Las habitaciones son super viejas y sucias y du confort es nulo.\n",
    "Las zonas comunes son preciosas eso si! Pero no es la mejor opciones para alojarse a la hora de conocer una ciudad tan bonita como La Habana.\"\"\"\n",
    "\n",
    "# Procesar el texto con SpaCy\n",
    "doc = nlp(texto)\n",
    "\n",
    "# for token in doc:\n",
    "#     # check token pos\n",
    "#     if token.pos_=='NOUN':\n",
    "#         # print token\n",
    "#         print(token.text)\n",
    "\n",
    "text = \"Los niños les encantan las galletas de crema.\"\n",
    "\n",
    "# create spacy \n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,'->',token.pos_,'->',token.dep_)\n",
    "\n",
    "for token in doc:\n",
    "    # extract subject\n",
    "    if (token.dep_=='nsubj'):\n",
    "        print(token.text)\n",
    "    # extract object\n",
    "    elif (token.dep_=='dobj'):\n",
    "        print(token.text)\n",
    "\n",
    "# from spacy import displacy \n",
    "# displacy.render(doc, style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for rule 2: noun(subject), verb, noun(object)\n",
    "def rule2(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # si el token es un verbo\n",
    "        if (token.pos_ in ['VERB']):\n",
    "            \n",
    "            phrase =''\n",
    "            \n",
    "            # extraer solo sujetos, sustantivos o pronombres\n",
    "            for sub_tok in token.lefts:\n",
    "                \n",
    "                if (sub_tok.dep_ in ['kkkkkkkkk']) or (sub_tok.pos_ in ['ADV']):\n",
    "                    \n",
    "                    phrase += ' '+ sub_tok.text\n",
    "                \n",
    "\n",
    "            phrase += ' '+token.text \n",
    "\n",
    "            for sub_tok in token.rights:\n",
    "                        \n",
    "                if (sub_tok.dep_ in ['nsubj']) and (sub_tok.pos_ in ['NOUN']):\n",
    "                        for sub_sub_tok in sub_tok.lefts:\n",
    "                            phrase += ' '+ sub_sub_tok.text\n",
    "                            phrase += ' '+ sub_tok.text\n",
    "                            \n",
    "            if len(phrase.split()) > 2:\n",
    "                sent.append(phrase)\n",
    "            \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for rule 3: noun(subject), verb, noun(object)\n",
    "def rule3(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # si el token es un verbo\n",
    "        if (token.pos_ in ['ADJ']):\n",
    "          \n",
    "            phrase =''\n",
    "            \n",
    "            # extraer solo sujetos, sustantivos o pronombres\n",
    "            for sub_tok in token.lefts:\n",
    "                \n",
    "                if (sub_tok.pos_ in ['NOUN','PRON','ADV','ADJ','DET','VEB']):\n",
    "                    \n",
    "                    phrase += ' '+ sub_tok.text\n",
    "                \n",
    "\n",
    "            phrase += ' '+token.text \n",
    "\n",
    "            for sub_tok in token.rights:\n",
    "                \n",
    "                if (sub_tok.pos_ in ['NOUN','PRON','ADV','ADJ','DET','VEB']):\n",
    "                    phrase += ' '+ sub_tok.text\n",
    "                            \n",
    "            if len(phrase.split()) > 2:\n",
    "                sent.append(phrase)\n",
    "            \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for rule 1: noun(subject), verb, noun(object)\n",
    "def rule1(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # si el token es un verbo\n",
    "        if (token.pos_ in ['VERB']):\n",
    "            \n",
    "            phrase =''\n",
    "            \n",
    "            # extraer solo sujetos, sustantivos o pronombres\n",
    "            for sub_tok in token.lefts:\n",
    "                \n",
    "                if (sub_tok.dep_ in ['nsubj','nsubjpass', 'cop']) and (sub_tok.pos_ in ['AUX','NOUN','PROPN','PRON']):\n",
    "                    if (sub_tok.dep_ in ['nsubj']) and (sub_tok.pos_ in ['NOUN']):\n",
    "                        for sub_sub_tok in sub_tok.lefts:\n",
    "                            phrase += ' '+ sub_sub_tok.text\n",
    "                            phrase += ' '+ sub_tok.text\n",
    "                    else:\n",
    "                        phrase += ' '+ sub_tok.text\n",
    "                \n",
    "\n",
    "            phrase += ' '+token.text \n",
    "\n",
    "            for sub_tok in token.rights:\n",
    "                        \n",
    "                if (sub_tok.dep_ in ['dobj','advcl']) and (sub_tok.pos_ in ['NOUN','PROPN','ADJ']):\n",
    "                                    \n",
    "                    phrase += ' '+sub_tok.text\n",
    "                    if len(phrase.split()) > 2:\n",
    "                        sent.append(phrase)\n",
    "                            \n",
    "            \n",
    "            \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for rule 4: noun(subject), verb, noun(object)\n",
    "def rule4(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # si el token es un verbo\n",
    "        if (token.pos_ in ['NOUN']):\n",
    "            \n",
    "            phrase =''\n",
    "            \n",
    "            # extraer solo sujetos, sustantivos o pronombres\n",
    "            for sub_tok in token.lefts:\n",
    "                \n",
    "                if (sub_tok.pos_ in ['ADJ']):\n",
    "                    \n",
    "                    phrase += ' '+ sub_tok.text\n",
    "                \n",
    "\n",
    "            phrase += ' '+token.text \n",
    "\n",
    "            for sub_tok in token.rights:\n",
    "                \n",
    "                if (sub_tok.pos_ in ['ADJ']):\n",
    "                    phrase += ' '+ sub_tok.text\n",
    "                            \n",
    "            if len(phrase.split()) > 1:\n",
    "                sent.append(phrase)\n",
    "    return sent\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for rule 5: noun(subject), verb, noun(object)\n",
    "def rule5(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # si el token es un verbo\n",
    "        if (token.pos_ in ['NOUN']):\n",
    "            \n",
    "            phrase =''\n",
    "            \n",
    "            # extraer solo sujetos, sustantivos o pronombres\n",
    "            for sub_tok in token.lefts:\n",
    "                \n",
    "                if (sub_tok.pos_ in ['ADJ','DET']):\n",
    "                    \n",
    "                    phrase += ' '+ sub_tok.text\n",
    "                \n",
    "\n",
    "            phrase += ' '+token.text \n",
    "\n",
    "            for sub_tok in token.rights:\n",
    "                \n",
    "                if (sub_tok.pos_ in ['ADJ','DET']):\n",
    "                    phrase += ' '+ sub_tok.text\n",
    "                            \n",
    "            if len(phrase.split()) > 2:\n",
    "                sent.append(phrase)\n",
    "    return sent\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for rule 6\n",
    "def rule6(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "\n",
    "    pat = []\n",
    "    \n",
    "    # iterate over tokens\n",
    "    for token in doc:\n",
    "        phrase = ''\n",
    "        # if the word is a subject noun or an object noun\n",
    "        if (token.pos_ == 'NOUN')\\\n",
    "            and (token.dep_ in ['dobj','pobj','nsubj','nsubjpass']):\n",
    "            print(token)\n",
    "            # iterate over the children nodes\n",
    "            for subtoken in token.children:\n",
    "                # if word is an adjective or has a compound dependency\n",
    "                if (subtoken.pos_ == 'ADJ') or (subtoken.dep_ == 'compound'):\n",
    "                    phrase += subtoken.text + ' '\n",
    "                    \n",
    "            if len(phrase)!=0:\n",
    "                phrase += token.text\n",
    "             \n",
    "        if  len(phrase)!=0:\n",
    "            pat.append(phrase)\n",
    "        \n",
    "    \n",
    "    return pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule2_mod(text,index):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "\n",
    "    phrase = ''\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        if token.i == index:\n",
    "            \n",
    "            for subtoken in token.children:\n",
    "                if (subtoken.pos_ == 'ADJ'):\n",
    "                    phrase += ' '+subtoken.text\n",
    "            break\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule 7 modified function\n",
    "def rule7(text, palabra):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # root word\n",
    "        if (token.text == palabra):\n",
    "            \n",
    "\n",
    "            phrase =''\n",
    "            \n",
    "            # only extract noun or pronoun subjects\n",
    "            for sub_tok in token.lefts:\n",
    "\n",
    "                phrase += ' ' + sub_tok.text\n",
    "\n",
    "                if sub_tok.dep_ in ['NOUN']:\n",
    "                        for x in sub_tok.rights:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase += ' ' + x.text\n",
    "                        \n",
    "                        for x in sub_tok.lefts:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase += ' ' + x.text \n",
    "\n",
    "                # save the root word of the word\n",
    "            phrase += ' '+token.text \n",
    "\n",
    "            # check for noun or pronoun direct objects\n",
    "            for sub_tok in token.rights:     \n",
    "                phrase += ' ' + sub_tok.text\n",
    "\n",
    "                if sub_tok.dep_ in ['NOUN']:\n",
    "                        for x in sub_tok.rights:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase += ' ' + x.text\n",
    "\n",
    "                        for x in sub_tok.lefts:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase += ' ' + x.text \n",
    "\n",
    "            head = token.head\n",
    "\n",
    "            # only extract noun or pronoun subjects\n",
    "            for sub_tok in head.lefts:\n",
    "\n",
    "                if sub_tok.text != token.text:\n",
    "                    phrase += ' ' + sub_tok.text\n",
    "\n",
    "                    if sub_tok.dep_ in ['NOUN']:\n",
    "                        for x in sub_tok.rights:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase += ' ' + x.text\n",
    "                        \n",
    "                        for x in sub_tok.lefts:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase += ' ' + x.text \n",
    "\n",
    "                # save the root word of the word\n",
    "            phrase += ' '+head.text \n",
    "\n",
    "            # check for noun or pronoun direct objects\n",
    "            for sub_tok in head.rights:\n",
    "                print(sub_tok.pos_)\n",
    "                if sub_tok.text != token.text:\n",
    "                    phrase += ' ' + sub_tok.text\n",
    "\n",
    "                    if sub_tok.pos_ in ['NOUN']:\n",
    "                        for x in sub_tok.rights:\n",
    "                            if x.pos_ in ['ADJ']:\n",
    "                                phrase += ' ' + x.text\n",
    "\n",
    "                        for x in sub_tok.lefts:\n",
    "                            if x.pos_ in ['ADJ']:\n",
    "                                phrase += ' ' + x.text    \n",
    "\n",
    "            sent.append(phrase)\n",
    "            \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"La habitación era limpia y muy comoda\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Las habitaciones eran super viejas y sucias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule 1 []\n",
      "Rule 2 []\n",
      "Rule 3 [' habitación limpia comoda']\n",
      "Rule 4 []\n",
      "Rule 5 []\n",
      "habitación\n",
      "Rule 6 []\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "rule7() missing 1 required positional argument: 'palabra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRule 5\u001b[39m\u001b[38;5;124m\"\u001b[39m, rule5(texto))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRule 6\u001b[39m\u001b[38;5;124m\"\u001b[39m, rule6(texto))\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRule 7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mrule7\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: rule7() missing 1 required positional argument: 'palabra'"
     ]
    }
   ],
   "source": [
    "print(\"Rule 1\", rule1(texto))\n",
    "print(\"Rule 2\", rule2(texto))\n",
    "print(\"Rule 3\", rule3(texto))\n",
    "print(\"Rule 4\", rule4(texto))\n",
    "print(\"Rule 5\", rule5(texto))\n",
    "print(\"Rule 6\", rule6(texto))\n",
    "print(\"Rule 7\", rule7(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m  \u001b[31mdep \u001b[34m  \u001b[32mSPACE\n",
      "\u001b[39mLas \u001b[31mobj \u001b[34mrecomiendo \u001b[32mPRON\n",
      "\u001b[39mrecomiendo \u001b[31mROOT \u001b[34mrecomiendo \u001b[32mVERB\n",
      "\u001b[39mmucho \u001b[31madvmod \u001b[34mrecomiendo \u001b[32mADV\n",
      "\u001b[39m, \u001b[31mpunct \u001b[34mdesayunos \u001b[32mPUNCT\n",
      "\u001b[39men \u001b[31mmark \u001b[34mdesayunos \u001b[32mADP\n",
      "\u001b[39mcuanto \u001b[31mfixed \u001b[34men \u001b[32mNOUN\n",
      "\u001b[39ma \u001b[31mfixed \u001b[34men \u001b[32mADP\n",
      "\u001b[39mdesayunos \u001b[31madvcl \u001b[34mrecomiendo \u001b[32mNOUN\n",
      "\u001b[39my \u001b[31mcc \u001b[34mhospitalidad \u001b[32mCCONJ\n",
      "\u001b[39mhospitalidad \u001b[31mconj \u001b[34mdesayunos \u001b[32mNOUN\n",
      "\u001b[39m10/10 \u001b[31mnummod \u001b[34mhospitalidad \u001b[32mNUM\n",
      "\u001b[39m! \u001b[31mpunct \u001b[34mrecomiendo \u001b[32mPUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from colorama import Fore\n",
    "\n",
    "BLUE = Fore.BLUE\n",
    "RED = Fore.RED\n",
    "RESET = Fore.RESET\n",
    "GREEN = Fore.GREEN\n",
    "\n",
    "# Cargar el modelo pre-entrenado en español de spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Definir la oración de ejemplo\n",
    "sentence = \"\"\" Las recomiendo mucho, en cuanto a desayunos y hospitalidad 10/10!\"\"\"\n",
    "\n",
    "# Procesar la oración con spacy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Imprimir las dependencias sintácticas de cada token en la oración\n",
    "for token in doc:\n",
    "    print(RESET + token.text, RED + token.dep_, BLUE + token.head.text, GREEN + token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AQUi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")  # Cargar el modelo en español de spaCy\n",
    "\n",
    "def extraccion_contexto(oracion, palabra_relevante):\n",
    "    doc = nlp(oracion)\n",
    "    token_palabra_relevante = None\n",
    "    for token in doc:\n",
    "        if token.text.lower() == palabra_relevante:\n",
    "            token_palabra_relevante = token\n",
    "            break\n",
    "\n",
    "    if not token_palabra_relevante:\n",
    "        return \"Palabra relevante no encontrada en la oración\"\n",
    "\n",
    "    modificadores = []\n",
    "    for token in doc:\n",
    "        if token.head == token_palabra_relevante and token.dep_ in [\"amod\", \"advmod\", \"nsubj\", \"nummod\", \"det\"]:\n",
    "            modificadores.append(token.text)\n",
    "    \n",
    "    return modificadores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gramas(oracion, palabra, numero):\n",
    "    palabras = oracion.split()  # Dividir la oración en palabras\n",
    "    index_palabra = []\n",
    "    pal = palabra.lower()+\",\"\n",
    "\n",
    "    for index, word in enumerate(palabras):\n",
    "\n",
    "        if word.lower() == palabra.lower() or word.lower() == pal:\n",
    "            index_palabra = index\n",
    "            break\n",
    "\n",
    "    palabras_anteriores = palabras[max(0, index_palabra - numero):index_palabra]\n",
    "    palabras_posteriores = palabras[index_palabra + 1:index_palabra + 1 + numero]\n",
    "\n",
    "    return palabras_anteriores + [palabra] + palabras_posteriores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule 8 modified function\n",
    "def rule8(text, palabra):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sent = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # root word\n",
    "        if (token.text == palabra):\n",
    "            \n",
    "\n",
    "            phrase = []\n",
    "            \n",
    "            # only extract noun or pronoun subjects\n",
    "            for sub_tok in token.lefts:\n",
    "\n",
    "                phrase.append(sub_tok.text)\n",
    "\n",
    "                if sub_tok.dep_ in ['NOUN']:\n",
    "                        for x in sub_tok.rights:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase.append(x.text)\n",
    "                        \n",
    "                        for x in sub_tok.lefts:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase.append(x.text) \n",
    "\n",
    "                # save the root word of the word\n",
    "            phrase.append(token.text) \n",
    "\n",
    "            # check for noun or pronoun direct objects\n",
    "            for sub_tok in token.rights:     \n",
    "                phrase.append(sub_tok.text)\n",
    "\n",
    "                if sub_tok.dep_ in ['NOUN']:\n",
    "                        for x in sub_tok.rights:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase.append(x.text)\n",
    "\n",
    "                        for x in sub_tok.lefts:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase.append(x.text) \n",
    "\n",
    "            head = token.head\n",
    "\n",
    "            # only extract noun or pronoun subjects\n",
    "            for sub_tok in head.lefts:\n",
    "\n",
    "                if sub_tok.text != token.text:\n",
    "                    phrase.append(sub_tok.text)\n",
    "\n",
    "                    if sub_tok.dep_ in ['NOUN']:\n",
    "                        for x in sub_tok.rights:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase.append(x.text)\n",
    "                        \n",
    "                        for x in sub_tok.lefts:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase.append(x.text) \n",
    "\n",
    "                # save the root word of the word\n",
    "            phrase.append(head.text) \n",
    "\n",
    "            # check for noun or pronoun direct objects\n",
    "            for sub_tok in head.rights:\n",
    "                if sub_tok.text != token.text:\n",
    "                    phrase.append(sub_tok.text)\n",
    "\n",
    "                    if sub_tok.pos_ in ['NOUN']:\n",
    "                        for x in sub_tok.rights:\n",
    "                            if x.pos_ in ['ADJ']:\n",
    "                                phrase.append(x.text)\n",
    "\n",
    "                        for x in sub_tok.lefts:\n",
    "                            if x.pos_ in ['ADJ']:\n",
    "                                phrase.append(x.text)    \n",
    "\n",
    "            sent.append(n_gramas(text,palabra,3))\n",
    "            sent.append(extraccion_contexto(text,palabra))\n",
    "            sent.append(phrase)\n",
    "            \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palabras_sin_repetir(lista_de_listas):\n",
    "    palabras = set()  # Utilizamos un conjunto para evitar palabras duplicadas\n",
    "    \n",
    "    for sublista in lista_de_listas:\n",
    "        for palabra in sublista:\n",
    "            palabras.add(palabra)  # Convertimos las palabras a minúsculas para evitar duplicados\n",
    "\n",
    "    return list(palabras)  # Convertimos el conjunto de palabras de nuevo a una lista y la devolvemos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination(oracion, list):\n",
    "    result = []\n",
    "    for token in oracion.split():\n",
    "        if (token in list) or (token[0:-1] in list):\n",
    "            if not token in result:\n",
    "                result.append(token)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['que', 'da', 'los', 'tours', 'internos', 'del', 'hotel']\n"
     ]
    }
   ],
   "source": [
    "oracion_ejemplo = \"\"\"La chica que da los tours internos del hotel es excelente en su trabajo y muy profesional y su jefa también, da muy buenas recomendaciones y son muy atentas!e\"\"\"\n",
    "palabra_relevante = \"tours\"\n",
    "\n",
    "modificadores = rule8(oracion_ejemplo, palabra_relevante)\n",
    "lista = palabras_sin_repetir(modificadores)\n",
    "print(combination(oracion_ejemplo,lista))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras=[\"desayunos\",\"comida\",\"servicio\",\"tour\",\"personal\",\"baño\"]\n",
    "opinion= \"Los tours\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "for palabra in palabras:\n",
    "            if palabra in opinion:\n",
    "                print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for palabra in palabras:\n",
    "    if re.search(r\"\\b\" + palabra + r\"\\b\", opinion):\n",
    "        print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer las dependencias del aspecto\n",
    "def extraer_dep(opinion, aspecto):\n",
    "    \n",
    "    # Se procesa la opinión con el modelo de lenguaje (nlp)\n",
    "    doc = nlp(opinion)\n",
    "    sent = []  # Lista para almacenar las frases extraídas\n",
    "    \n",
    "    for token in doc:  # Iterar sobre cada token (palabra) en la opinión\n",
    "\n",
    "        # Si el token coincide con el aspecto buscado\n",
    "        if (token.text == aspecto):\n",
    "            phrase = []  # Lista para almacenar las palabras relacionadas al aspecto\n",
    "\n",
    "            # Extraer dependencias (sustantivos o pronombres) a la izquierda del aspecto\n",
    "            for sub_tok in token.lefts:\n",
    "                phrase.append(sub_tok.text)\n",
    "\n",
    "                # Si es un sustantivo, buscar adjetivos relacionados\n",
    "                if sub_tok.dep_ in ['NOUN']:\n",
    "\n",
    "                    for x in sub_tok.lefts:\n",
    "                        if x.dep_ in ['ADJ']:\n",
    "                            phrase.append(x.text)\n",
    "\n",
    "                    for x in sub_tok.rights:\n",
    "                        if x.dep_ in ['ADJ']:\n",
    "                            phrase.append(x.text)\n",
    "                    \n",
    "            # Agregar el aspecto a la frase\n",
    "            phrase.append(token.text) \n",
    "\n",
    "            # Extraer dependencias (sustantivos o pronombres) a la derecha del aspecto\n",
    "            for sub_tok in token.rights:     \n",
    "                phrase.append(sub_tok.text)\n",
    "\n",
    "                # Si es un sustantivo, buscar adjetivos relacionados\n",
    "                for x in sub_tok.lefts:\n",
    "                        if x.dep_ in ['ADJ']:\n",
    "                            phrase.append(x.text) \n",
    "\n",
    "                if sub_tok.dep_ in ['NOUN']:\n",
    "                    for x in sub_tok.rights:\n",
    "                        if x.dep_ in ['ADJ']:\n",
    "                            phrase.append(x.text)\n",
    "\n",
    "                    \n",
    "            head = token.head  # Obtener la palabra principal del aspecto\n",
    "\n",
    "            # Extraer sujetos relacionados a la palabra principal (excluyendo el aspecto)\n",
    "            for sub_tok in head.lefts:\n",
    "                if sub_tok.text != token.text:\n",
    "                    phrase.append(sub_tok.text)\n",
    "\n",
    "                    # Si el sujeto es un sustantivo, buscar adjetivos relacionados\n",
    "                    if sub_tok.dep_ in ['NOUN']:\n",
    "                        for x in sub_tok.lefts:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase.append(x.text)\n",
    "\n",
    "                        for x in sub_tok.rights:\n",
    "                            if x.dep_ in ['ADJ']:\n",
    "                                phrase.append(x.text)\n",
    "                        \n",
    "                        \n",
    "\n",
    "            # Agregar la palabra principal a la frase\n",
    "            phrase.append(head.text) \n",
    "\n",
    "            # Extraer objetos directos relacionados a la palabra principal (excluyendo el aspecto)\n",
    "            for sub_tok in head.rights:\n",
    "                if sub_tok.text != token.text:\n",
    "                    phrase.append(sub_tok.text)\n",
    "\n",
    "                    # Si el objeto directo es un sustantivo, buscar adjetivos relacionados\n",
    "                    if sub_tok.pos_ in ['NOUN']:  # Aquí se usa pos_ en lugar de dep_\n",
    "                        for x in sub_tok.lefts:\n",
    "                            if x.pos_ in ['ADJ']:\n",
    "                                phrase.append(x.text) \n",
    "\n",
    "                        for x in sub_tok.rights:\n",
    "                            if x.pos_ in ['ADJ']:\n",
    "                                phrase.append(x.text)\n",
    "\n",
    "            # Agregar n-gramas, contexto y la frase extraída a la lista de resultados\n",
    "            sent.append(n_gramas(opinion,aspecto,3))\n",
    "            sent.append(extraccion_contexto(opinion,aspecto))\n",
    "            sent.append(phrase)\n",
    "    print(sent)\n",
    "            \n",
    "    return sent\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
